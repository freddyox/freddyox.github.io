<!DOCTYPE HTML>
<!--
    Massively by HTML5 UP
    html5up.net | @ajlkn
    Jekyll integration by somiibo.com
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  -->
<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

<title>Naive Bayes Classifier</title>
<meta name="description" content="">

<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/assets/icon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:4000/assets/icon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:4000/assets/icon/favicon-16x16.png">
<link rel="manifest" href="http://localhost:4000/assets/icon/manifest.json">
<link rel="mask-icon" href="http://localhost:4000/assets/icon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="http://localhost:4000/assets/icon/favicon.ico">
<meta name="msapplication-config" content="http://localhost:4000/assets/icon/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

<!-- CSS -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css" />
<noscript><link rel="stylesheet" href="http://localhost:4000/assets/css/noscript.css" /></noscript>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
<!--<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } } }); </script> -->

  </head>
  <body class="is-loading">

    <!-- Wrapper -->
    <div id="wrapper" class="fade-in">

      <!-- Header -->
      <header id="header">
        <a href="http://localhost:4000/" class="logo">Projects and Research Interests</a>
      </header>

      <!-- Nav -->
      <nav id="nav">
        <ul class="links">
  <li class=""><a href="http://localhost:4000/">Home</a></li>
  <li class=""><a href="http://localhost:4000/about/">About</a></li>
  <li class=" active "><a href="http://localhost:4000/blog/">All Projects</a></li>
  <li class=""><a href="http://localhost:4000/categories/">Categories</a></li>
  <li class=""><a href="http://localhost:4000/elements/">Useful References</a></li>
</ul>

      </nav>
      <!-- Main -->
      <div id="main">
        <section class="post">
    	  <header class="major">
      	    <span class="date">13 Oct 2018</span>
      	    <h1>Naive Bayes Classifier</h1>
      	    <p>A probabilistic classifier using Gaussian class-conditionals</p>
      	  </header>
      	  <div class="image main"><img src="" alt=""></div>
      	  <p><figure>
  <img src="/images/bayes_classifier/bayesian_example.png" alt="" height="99%" width="99%" />
  <figcaption>Fig. 1 - There are three data sets labeled A, B, and C. The goal
  is to understand the class distributions such that when new unlabeled data are presented
  (red circles labeled 1-3), a class may be assigned based on probability.
  In this example, the new data point labeled as 1 has an 84% chance
  of belonging to Class A and only a 14% chance of belonging to Class C; therefore, this
  point would be assigned as a member of Class A.
  The procedure is described in more detail below.</figcaption>
</figure>

<p></p>
<h2 id="overview-of-the-problem">Overview of the problem</h2>
<p>This topic is thoroughly discussed elsewhere; however, for completeness Iâ€™d like to
simply state the problem, then to explore the Bayes classifier approach.
Suppose we have <script type="math/tex">N</script> training objects, which are described by <script type="math/tex">D</script>-dimensional
features <script type="math/tex">\boldsymbol{x}_1</script>, <script type="math/tex">\dots</script>, <script type="math/tex">\boldsymbol{x}_N</script>. Each object has a
class label <script type="math/tex">C_k</script>, <i>e.g.</i> Fig. 1 displays three classes A, B, and C for
a total of three possible outcomes.
Our goal, then, is to compute predictive probabilities that an unseen object
<script type="math/tex">x_{\textrm{new}}</script> belongs to a particular class <script type="math/tex">C_k</script>.</p>

<p>In the language of Bayesian probability, we are interested in calculating the posterior
for each of the possible <script type="math/tex">C</script> classes, which will then guide the decision-making
process by accepting the class with the largest probability. The posterior, or
the probability that an unseen data point belongs to class <script type="math/tex">C_k</script>, is defined as</p>

<script type="math/tex; mode=display">p(C_k | \boldsymbol{x}) = \frac{p(\boldsymbol{x}|C_k) p(C_k)}{p(\boldsymbol{x})}
               = \frac{ \textrm{likelihood} \times \textrm{prior} }{ \textrm{marginal} }.</script>

<p>In order to compute the posteriors, functional forms for the likelihoods, the priors, and
the marginal (or evidence) are required.
Class-sized priors are used in this example, <i>i.e.</i> <script type="math/tex">p(C_k) = N_k / N</script> where
<script type="math/tex">N_k</script> is the number of training objects in class <script type="math/tex">k</script> and <script type="math/tex">N</script> is the total
number of training objects. Gaussian class-conditionals are used for the likelihood functions
for each class. More plainly, the data for class <script type="math/tex">k</script> may be analyzed in order to extract
the mean and covariance matrix, denoted as <script type="math/tex">\boldsymbol{\mu}_k</script> and
<script type="math/tex">\boldsymbol{\Sigma}_k</script>, which may be inserted into a multivariate Gaussian to calculate
probabilities. The maximum likelihood estimates are</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\boldsymbol{\mu}_k   &= \frac{1}{N_k} \sum_{n=1}^{N_k} \boldsymbol{x}_n \\
\boldsymbol{\Sigma}_k &= \frac{1}{N_k} \sum_{n=1}^{N_k} \left( \boldsymbol{x}_n - \boldsymbol{\mu}_k \right) \left( \boldsymbol{x}_n - \boldsymbol{\mu}_k \right)^{\textrm{T}},
\end{align} %]]></script>

<p>where the superscript T represents vector-transpose. For example, if one were to examine
Class A in Fig. 1, the means in the <script type="math/tex">x_1</script> and <script type="math/tex">x_2</script> directions need to be computed
in addition to the covariance matrix (which gives the variances in both features as well
as the correlation, if any). For each class, the multivariate Gaussian probability may be calculated
as (generalized to <script type="math/tex">D</script>-dimensions)</p>

<script type="math/tex; mode=display">p(\boldsymbol{x}) = \frac{1}{ (2\pi)^{D/2} |\boldsymbol{\Sigma}|^{1/2}}
                    \exp{\left[-\frac{1}{2}\left( \boldsymbol{x}-\boldsymbol{\mu}\right)^{\textrm{T}} \boldsymbol{\Sigma}^{-1} \left(  \boldsymbol{x}-\boldsymbol{\mu} \right)\right]},</script>

<p>where <script type="math/tex">|\boldsymbol{\Sigma}|</script> is the determinant and <script type="math/tex">\boldsymbol{\Sigma}^{-1}</script> is the inverse
of the <script type="math/tex">D \times D</script> covariance matrix. For example in Fig. 1,
<script type="math/tex">\boldsymbol{\Sigma}</script> is a <script type="math/tex">2\times2</script> symmetric matrix where only three parameters
need to be determined (the off-diagonals are equal), and <script type="math/tex">\boldsymbol{\mu}</script> is a two-dimensional
vector; therefore, the argument of the exponential is simply a number and easily calculable.
Lastly, the marginal probability is a weighted sum (over all classes)
of the above probabilities and acts as a normalization term, <i>i.e.</i> the sum over all classes
of the likelihood<script type="math/tex">\times</script>prior.
This will explicitly be shown below.</p>

<h2 id="generating-pseudo-training-data">Generating pseudo-training data</h2>
<p>We will work with only two features, denoted by <script type="math/tex">x_1</script> and <script type="math/tex">x_2</script>, for visualization
reasons. The first order of business is generating data that may be fit or described by
arbitrarily oriented two-dimensional Gaussians, which requires the ability to generate
correlated data. Assuming we can randomly sample from a one-dimensional Gaussian
of a defined mean and variance, or <script type="math/tex">\mathcal{N}(\mu,\sigma^2)</script>, we can generate
normally independent values for both features, denoted as <script type="math/tex">z_{x_1}</script> and <script type="math/tex">z_{x_2}</script>.
In order to add a correlation <script type="math/tex">\rho</script>, the sampled values become</p>

<script type="math/tex; mode=display">x_1 = z_{x_1} \hspace{1cm} \mathrm{and} \hspace{1cm} x_2=\rho z_{x_1} + \sqrt{1-\rho^2} z_{x_2}.</script>

<p>If <script type="math/tex">\rho=0,1</script> then there is no correlation; positive (negative) <script type="math/tex">\rho</script> results in
positive (negative) slopes in the distribution. In Fig. 1, Class A uses <script type="math/tex">\rho=0.25</script> and
Class C uses <script type="math/tex">\rho=-0.5</script>.</p>

<h2 id="classification">Classification</h2>
<p>The training data, which may be seen by the blue triangles, black crosses, and green diamonds
in Fig. 1, need to be analyzed in order to extract the mean
<script type="math/tex">\boldsymbol{\mu} = ( \overline{x}_1, \overline{x}_2)^{\textrm{T}}</script>
and the covariance matrix <script type="math/tex">\boldsymbol{\Sigma}</script> for each class.
Assuming Gaussian class-conditionals,
we may now calculate the 2-dimensional probability for the three classes, which
may be seen by the contours in Fig. 1. The contours are built by scanning through
the <script type="math/tex">x_1/x_2</script> space and constructing a two-dimensional
histogram where the color-weighting is the normalized multivariate Gaussian probability.
We now have all the parts to make some predictions for unseen data.</p>

<h2 id="predictions">Predictions</h2>
<p>Fig. 1 has three unseen data labeled as 1-3. The points are randomly chosen using the Class
probability distributions, <i>i.e.</i> point 1 is sampled from the Class A normal distribution;
this is done on purpose as I know where the data came from and I want to see how the calculation
classifies this object based on the above assumptions. In this case, the red circle labeled
as 1 has an 84% chance of belonging to Class A, a 1% chance of belonging to Class B, and a
14% chance of belonging to Class C. This unseen point would therefore be classified as a member
of Class A. Here is another example in which the calculations are explicitly shown, see the
figure and table below.</p>
<figure>
  <img src="/images/bayes_classifier/bayesian_example2.png" alt="" height="99%" width="99%" />
</figure>

<table>
  <thead>
    <tr>
      <th>Point #</th>
      <th>Class</th>
      <th>Likelihood</th>
      <th>Prior</th>
      <th>Marginal</th>
      <th>Posterior (%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>A</td>
      <td>0.9754</td>
      <td>0.2188</td>
      <td>0.2227</td>
      <td>95.813</td>
    </tr>
    <tr>
      <td>1</td>
      <td>B</td>
      <td>0.0298</td>
      <td>0.3125</td>
      <td>0.2227</td>
      <td>4.181</td>
    </tr>
    <tr>
      <td>1</td>
      <td>C</td>
      <td>0.0000</td>
      <td>0.4688</td>
      <td>0.2227</td>
      <td>0.006</td>
    </tr>
    <tr>
      <td>2</td>
      <td>A</td>
      <td>0.0000</td>
      <td>0.2188</td>
      <td>0.0507</td>
      <td>0.000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>B</td>
      <td>0.1621</td>
      <td>0.3125</td>
      <td>0.0507</td>
      <td>100.000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>C</td>
      <td>0.0000</td>
      <td>0.4688</td>
      <td>0.0507</td>
      <td>0.000</td>
    </tr>
    <tr>
      <td>3</td>
      <td>A</td>
      <td>0.2168</td>
      <td>0.2188</td>
      <td>0.0724</td>
      <td>65.394</td>
    </tr>
    <tr>
      <td>3</td>
      <td>B</td>
      <td>0.0564</td>
      <td>0.3125</td>
      <td>0.0724</td>
      <td>24.309</td>
    </tr>
    <tr>
      <td>3</td>
      <td>C</td>
      <td>0.0159</td>
      <td>0.4688</td>
      <td>0.0724</td>
      <td>10.297</td>
    </tr>
  </tbody>
</table>

<p>Recall that the priors are calculated as the fraction of training objects in
the class to the total number of training objects; therefore, since each class
has a different number of points, the priors are different (Class A has 70,
Class B has 100, and Class C has 150). The likelihood is calculated as the
two-dimensional Gaussian using the mean and covariance matrix for that particular
class, which is determined from the training data. The marginal is simply the sum
of likelihood<script type="math/tex">\times</script>prior for all three classes. The posterior, or the predictive
probability of interest, is calculated as likelihood<script type="math/tex">\times</script>prior/marginal.
The point labeled as 3, which originally has been sampled from the Class C probability distribution,
is determined to be a member of Class A, which is arguably incorrect.</p>

<p>The classification may be summarized by the following histograms. One may scan through
the feature space and calculate the posteriors for all classes; the maximum posterior
then determines the classification, see the figure below where the light blue represents a situation
where Class A is a maximum, tan represents when Class B is a maximum, and dark red represents
when Class C is a maximum. Each class has a small gradient effect which is a result of
the value of the class posterior when itâ€™s determined to be the maximum. Lastly,
see Fig. 3 for a summary of the three classes where in this case the color weighting
is equal to the posterior probability.</p>
<figure>
  <img src="/images/bayes_classifier/which_class_is_max_better.png" alt="" height="99%" width="99%" />
    <figcaption>Fig. 2 - Summarizing the classification within the feature space, where
    light blue means that the Class A posterior is a maximum, tan is Class B, and dark red
    is Class C. The binning in both directions is 1/150. Note that this depends
    on the training data, which is randomly generated, and does not reflect the exact
    nature of the above plots (but it is close).</figcaption>
</figure>
<p></p>
<figure>
  <img src="/images/bayes_classifier/bayesian_summary.png" alt="" height="99%" width="99%" />
    <figcaption>Fig. 3 - Summarizing the classification within the feature space. The
    color weighting is the posterior probability for the three classes, and white represents
    a posterior of zero (or extremely close to it). Notice how these plots are
    slightly different than Fig. 2, and is a consequence of fitting to randomly
    generated training data (seen in the top-left panel) which slightly alters the predictive
    mean/covariance matrix. In this case, Class A sampled some
    data that extend above the Class C negatively correlated data, and explains the
    features seen in the top-right panel.</figcaption>
</figure>

</p>
      	</section>	

	
	<div id="disqus_thread"></div>
	<script>
	  /*
	  var disqus_config = function () {
	  this.page.url = "http://localhost:4000/blog/bayesian_classifier/";
	  this.page.identifier = "/blog/bayesian_classifier/";
	  };
	  */
	  (function() { // DON'T EDIT BELOW THIS LINE
	  var d = document, s = d.createElement('script');
	  s.src = 'https://freddyox-github-io.disqus.com/embed.js';
	  s.setAttribute('data-timestamp', +new Date());
	  (d.head || d.body).appendChild(s);
	  })();
	</script>
	<noscript>Please enable JavaScript to view the
	  <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	

	<!-- Footer -->
	<footer>
          <ul class="actions">
            <li><a href="http://localhost:4000/blog/" class="button">All Projects</a></li>
          </ul>
	</footer>
      </div>

      <!-- Footer -->
      <!--<footer id="footer">
  <section>
    <form method="POST" action="https://formspree.io/richard.obrecht@uconn.edu">
      <div class="field">
        <label for="name">Name</label>
        <input type="text" name="name" id="name" />
      </div>
      <div class="field">
        <label for="email">Email</label>
        <input type="text" name="email" id="email" />
      </div>
      <div class="field">
        <label for="message">Message</label>
        <textarea name="message" id="message" rows="3"></textarea>
      </div>
      <ul class="actions">
        <li><input type="submit" value="Send Message" /></li>
      </ul>
    </form>
  </section>
  <section class="split contact">
    <section class="alt">
      <h3>Location</h3>
      <p>Newport News, VA</p>
    </section>
    <section>
      <h3>Email</h3>
      <p><a href="mailto:richard.obrecht@uconn.edu">richard.obrecht@uconn.edu</a></p>
    </section>
  </section>
</footer>
<!-- Copyright -->
<div id="copyright">
  <ul>&copy;
    <script type="text/javascript">
      document.write(new Date().getFullYear());
    </script>
    Richard F. Obrecht</ul>
</div>
-->

    </div>

    <!-- Scripts -->
    <!-- DYN -->
<script src="http://localhost:4000/assets/js/jquery.min.js"></script>
<script src="http://localhost:4000/assets/js/jquery.scrollex.min.js"></script>
<script src="http://localhost:4000/assets/js/jquery.scrolly.min.js"></script>
<script src="http://localhost:4000/assets/js/skel.min.js"></script>
<script src="http://localhost:4000/assets/js/util.js"></script>
<script src="http://localhost:4000/assets/js/main.js"></script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=default"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'default');
</script>

  </body>
</html>
