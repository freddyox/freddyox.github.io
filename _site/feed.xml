<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.6.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-10-26T19:50:15-04:00</updated><id>http://localhost:4000/</id><title type="html">Projects and Research Interests</title><subtitle>Projects and Research Interests</subtitle><entry><title type="html">K-means clustering</title><link href="http://localhost:4000/blog/Kmeans/" rel="alternate" type="text/html" title="K-means clustering" /><published>2018-10-15T00:00:00-04:00</published><updated>2018-10-15T00:00:00-04:00</updated><id>http://localhost:4000/blog/Kmeans</id><content type="html" xml:base="http://localhost:4000/blog/Kmeans/">&lt;figure&gt;
  &lt;img src=&quot;/images/kmeans/4Means_example_success.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
  &lt;figcaption&gt;Fig. 1 - Unlabeled data (left) is clustered or grouped into
                       4 categories (right), which are arbitrarily labeled as
		       A, B, C, and D. An animation of the algorithm may be seen below.
		       Lastly, the algorithm is applied to Old Faithful geyser data in order to
		       make an educated guess on the wait-time for the next eruption.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#kmeans&quot;&gt;K-means clustering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#animation&quot;&gt;Animation of the algorithm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#old_faithful&quot;&gt;Application to Old Faithful&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;overview-of-the-problem-&quot;&gt;Overview of the problem &lt;a class=&quot;anchor&quot; id=&quot;kmeans&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are given &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; objects each of which have features &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}=\left(x_1, x_2\right)&lt;/script&gt; but
no class labels. Therefore, an unsupervised algorithm such as &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;-means clustering
may be utilized in order to group data of similar features; the output of my implementation
may be seen by Fig. 1.
The phrase “similar features” is quite ambiguous, however the
most naive interpretation may be thought in terms of distance. In other words, similar objects
are defined to be objects that are close together in terms of the square distance:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D^{2}=(\boldsymbol{x}_i - \boldsymbol{x}_j)^{T}(\boldsymbol{x}_i - \boldsymbol{x}_j)
 = (x_{i1} - x_{j1})^{2} + (x_{i2} - x_{j2})^{2},&lt;/script&gt;

&lt;p&gt;where the simplification is a result of only working with two features, and the superscript
&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; refers to vector-transpose. In order to automate this procedure for a data set
that looks like the left-panel of Fig. 1, then a cluster needs to be defined more clearly.&lt;/p&gt;

&lt;h2 id=&quot;clustering-algorithm--&quot;&gt;Clustering algorithm  &lt;a class=&quot;anchor&quot; id=&quot;animation&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;-means, there are &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; clusters (&lt;i&gt;e.g.&lt;/i&gt; 4 clusters in Fig. 1)
that are defined by the feature-space mean using the data points constituting the cluster
in the calculation, &lt;i&gt;i.e.&lt;/i&gt; in the &lt;script type=&quot;math/tex&quot;&gt;(x_1,x_2)&lt;/script&gt; space one needs to
calculate the average in both variables but only using data points that have been determined
to be within the cluster.
The argument is circular; we need a mean-point to define a cluster and yet the cluster needs
points to calculate the mean. In order to overcome this strange logic, the mean-vector
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\mu}&lt;/script&gt; for the &lt;script type=&quot;math/tex&quot;&gt;k^{th}&lt;/script&gt; cluster is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mu}_k = \frac{\sum_n z_{nk} \boldsymbol{x}_n}{\sum_n z_{nk}},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;z_{nk}&lt;/script&gt; is a binary variable which is equal to 1 if the data
object is a part of cluster &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, and zero otherwise;
this is simply a weighted average, and keeps track of how many objects
are in cluster &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;. Randomly initialize &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\mu}&lt;/script&gt; for the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; clusters.
It is best to keep the randomization within the bounds of the data, otherwise a
cluster may not develop. Randomly assign all unlabeled data to a cluster; therefore, a book-keeping
device (an integer ranging from 0 to 3 in Fig. 1 for example)
is required for each data object to track cluster assignment.
The following procedures are then performed iteratively:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For each data object, calculate the distance between &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}_n&lt;/script&gt; and
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\mu}_k&lt;/script&gt; for the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; clusters, &lt;i&gt;i.e.&lt;/i&gt; find &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; that minimizes
&lt;script type=&quot;math/tex&quot;&gt;D=\sqrt{(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^{T}(\boldsymbol{x}_n - \boldsymbol{\mu}_k)}&lt;/script&gt;.
The data point is assumed to be a member of the cluster
that yields the smallest distance, &lt;i&gt;i.e.&lt;/i&gt; update the data-object’s book-keeping
device to reflect this change, if any. If no updates are required, then exit the iterative-loop.&lt;/li&gt;
  &lt;li&gt;If we are here, then the book-keeping device of at least one data-object has been updated; therefore
update &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\mu}_k&lt;/script&gt; by recalculating the weighted-averages to reflect this change.&lt;/li&gt;
  &lt;li&gt;Return to procedure 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/kmeans/animation_loop.gif&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig. 2 - An animation of the algorithm where the black stars represent
  the means of the five clusters.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;In the above example, the unlabeled data has been classified or organized
into five clusters (blue square, green triangle, red circle, purple
upside-down triangle, copper cross). The black stars, which represent &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\mu}_k&lt;/script&gt;
or the mean of the &lt;script type=&quot;math/tex&quot;&gt;k^{th}&lt;/script&gt; cluster, move around due to the above update rule; the algorithm
stops once the data-object’s book-keeping device is no longer updated.&lt;/p&gt;

&lt;h2 id=&quot;application-to-old-faithful-&quot;&gt;Application to Old Faithful &lt;a class=&quot;anchor&quot; id=&quot;old_faithful&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Data from more than 270 observations on the Old Faithful geyser
&lt;a href=&quot;https://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat&quot;&gt;may be found here&lt;/a&gt;.
We are given the duration of the current eruption in addition to the waiting time
for the next eruption. The data distribution may be seen by Fig. 3, which is clustered
into two groups, labeled long and short, using K-means. In short, the eruption time is either
short &lt;script type=&quot;math/tex&quot;&gt;(\lesssim 3 \textrm{min})&lt;/script&gt; or long &lt;script type=&quot;math/tex&quot;&gt;(\gtrsim 3 \textrm{min})&lt;/script&gt;,
which means that we have to wait a
short (roughly 55 min) or long (roughly 80 min) time until the next eruption. Note this
data is almost 30 years old, and consequently current predictions may differ. The uncertainty
in this prediction is roughly &lt;script type=&quot;math/tex&quot;&gt;2-3 \sigma&lt;/script&gt;, which comes out to be about &lt;script type=&quot;math/tex&quot;&gt;\pm 10-15&lt;/script&gt; minutes;
so get there early just in case!&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/kmeans/old_faithful_summary.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig. 3 - Applying K-means (using Python's scikit-learn for practice) to Old Faithful data.
              The algorithm finds two clusters, labeled as long and short, which
	      may be used to predict the time one needs to wait until the next eruption given
	      the duration of the current eruption.&lt;/figcaption&gt;
&lt;/figure&gt;</content><summary type="html">An unsupervised clustering algorithm</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Naive Bayes Classifier</title><link href="http://localhost:4000/blog/bayesian_classifier/" rel="alternate" type="text/html" title="Naive Bayes Classifier" /><published>2018-10-13T00:00:00-04:00</published><updated>2018-10-13T00:00:00-04:00</updated><id>http://localhost:4000/blog/bayesian_classifier</id><content type="html" xml:base="http://localhost:4000/blog/bayesian_classifier/">&lt;figure&gt;
  &lt;img src=&quot;/images/bayes_classifier/bayesian_example.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig. 1 - There are three data sets labeled A, B, and C. The goal
  is to understand the class distributions such that when new unlabeled data are presented
  (red circles labeled 1-3), a class may be assigned based on probability.
  In this example, the new data point labeled as 1 has an 84% chance
  of belonging to Class A and only a 14% chance of belonging to Class C; therefore, this
  point would be assigned as a member of Class A.
  The procedure is described in more detail below.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview-of-the-problem&quot;&gt;Overview of the problem&lt;/h2&gt;
&lt;p&gt;This topic is thoroughly discussed elsewhere; however, for completeness I’d like to
simply state the problem, then to explore the Bayes classifier approach.
Suppose we have &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; training objects, which are described by &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;-dimensional
features &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\dots&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}_N&lt;/script&gt;. Each object has a
class label &lt;script type=&quot;math/tex&quot;&gt;C_k&lt;/script&gt;, &lt;i&gt;e.g.&lt;/i&gt; Fig. 1 displays three classes A, B, and C for
a total of three possible outcomes.
Our goal, then, is to compute predictive probabilities that an unseen object
&lt;script type=&quot;math/tex&quot;&gt;x_{\textrm{new}}&lt;/script&gt; belongs to a particular class &lt;script type=&quot;math/tex&quot;&gt;C_k&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In the language of Bayesian probability, we are interested in calculating the posterior
for each of the possible &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; classes, which will then guide the decision-making
process by accepting the class with the largest probability. The posterior, or
the probability that an unseen data point belongs to class &lt;script type=&quot;math/tex&quot;&gt;C_k&lt;/script&gt;, is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(C_k | \boldsymbol{x}) = \frac{p(\boldsymbol{x}|C_k) p(C_k)}{p(\boldsymbol{x})}
               = \frac{ \textrm{likelihood} \times \textrm{prior} }{ \textrm{marginal} }.&lt;/script&gt;

&lt;p&gt;In order to compute the posteriors, functional forms for the likelihoods, the priors, and
the marginal (or evidence) are required.
Class-sized priors are used in this example, &lt;i&gt;i.e.&lt;/i&gt; &lt;script type=&quot;math/tex&quot;&gt;p(C_k) = N_k / N&lt;/script&gt; where
&lt;script type=&quot;math/tex&quot;&gt;N_k&lt;/script&gt; is the number of training objects in class &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; is the total
number of training objects. Gaussian class-conditionals are used for the likelihood functions
for each class. More plainly, the data for class &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; may be analyzed in order to extract
the mean and covariance matrix, denoted as &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\mu}_k&lt;/script&gt; and
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\Sigma}_k&lt;/script&gt;, which may be inserted into a multivariate Gaussian to calculate
probabilities. The maximum likelihood estimates are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\boldsymbol{\mu}_k   &amp;= \frac{1}{N_k} \sum_{n=1}^{N_k} \boldsymbol{x}_n \\
\boldsymbol{\Sigma}_k &amp;= \frac{1}{N_k} \sum_{n=1}^{N_k} \left( \boldsymbol{x}_n - \boldsymbol{\mu}_k \right) \left( \boldsymbol{x}_n - \boldsymbol{\mu}_k \right)^{\textrm{T}},
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the superscript T represents vector-transpose. For example, if one were to examine
Class A in Fig. 1, the means in the &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt; directions need to be computed
in addition to the covariance matrix (which gives the variances in both features as well
as the correlation, if any). For each class, the multivariate Gaussian probability may be calculated
as (generalized to &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;-dimensions)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\boldsymbol{x}) = \frac{1}{ (2\pi)^{D/2} |\boldsymbol{\Sigma}|^{1/2}}
                    \exp{\left[-\frac{1}{2}\left( \boldsymbol{x}-\boldsymbol{\mu}\right)^{\textrm{T}} \boldsymbol{\Sigma}^{-1} \left(  \boldsymbol{x}-\boldsymbol{\mu} \right)\right]},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;|\boldsymbol{\Sigma}|&lt;/script&gt; is the determinant and &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\Sigma}^{-1}&lt;/script&gt; is the inverse
of the &lt;script type=&quot;math/tex&quot;&gt;D \times D&lt;/script&gt; covariance matrix. For example in Fig. 1,
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\Sigma}&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;2\times2&lt;/script&gt; symmetric matrix where only three parameters
need to be determined (the off-diagonals are equal), and &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\mu}&lt;/script&gt; is a two-dimensional
vector; therefore, the argument of the exponential is simply a number and easily calculable.
Lastly, the marginal probability is a weighted sum (over all classes)
of the above probabilities and acts as a normalization term, &lt;i&gt;i.e.&lt;/i&gt; the sum over all classes
of the likelihood&lt;script type=&quot;math/tex&quot;&gt;\times&lt;/script&gt;prior.
This will explicitly be shown below.&lt;/p&gt;

&lt;h2 id=&quot;generating-pseudo-training-data&quot;&gt;Generating pseudo-training data&lt;/h2&gt;
&lt;p&gt;We will work with only two features, denoted by &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;, for visualization
reasons. The first order of business is generating data that may be fit or described by
arbitrarily oriented two-dimensional Gaussians, which requires the ability to generate
correlated data. Assuming we can randomly sample from a one-dimensional Gaussian
of a defined mean and variance, or &lt;script type=&quot;math/tex&quot;&gt;\mathcal{N}(\mu,\sigma^2)&lt;/script&gt;, we can generate
normally independent values for both features, denoted as &lt;script type=&quot;math/tex&quot;&gt;z_{x_1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;z_{x_2}&lt;/script&gt;.
In order to add a correlation &lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt;, the sampled values become&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_1 = z_{x_1} \hspace{1cm} \mathrm{and} \hspace{1cm} x_2=\rho z_{x_1} + \sqrt{1-\rho^2} z_{x_2}.&lt;/script&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;\rho=0,1&lt;/script&gt; then there is no correlation; positive (negative) &lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt; results in
positive (negative) slopes in the distribution. In Fig. 1, Class A uses &lt;script type=&quot;math/tex&quot;&gt;\rho=0.25&lt;/script&gt; and
Class C uses &lt;script type=&quot;math/tex&quot;&gt;\rho=-0.5&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;classification&quot;&gt;Classification&lt;/h2&gt;
&lt;p&gt;The training data, which may be seen by the blue triangles, black crosses, and green diamonds
in Fig. 1, need to be analyzed in order to extract the mean
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\mu} = ( \overline{x}_1, \overline{x}_2)^{\textrm{T}}&lt;/script&gt;
and the covariance matrix &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\Sigma}&lt;/script&gt; for each class.
Assuming Gaussian class-conditionals,
we may now calculate the 2-dimensional probability for the three classes, which
may be seen by the contours in Fig. 1. The contours are built by scanning through
the &lt;script type=&quot;math/tex&quot;&gt;x_1/x_2&lt;/script&gt; space and constructing a two-dimensional
histogram where the color-weighting is the normalized multivariate Gaussian probability.
We now have all the parts to make some predictions for unseen data.&lt;/p&gt;

&lt;h2 id=&quot;predictions&quot;&gt;Predictions&lt;/h2&gt;
&lt;p&gt;Fig. 1 has three unseen data labeled as 1-3. The points are randomly chosen using the Class
probability distributions, &lt;i&gt;i.e.&lt;/i&gt; point 1 is sampled from the Class A normal distribution;
this is done on purpose as I know where the data came from and I want to see how the calculation
classifies this object based on the above assumptions. In this case, the red circle labeled
as 1 has an 84% chance of belonging to Class A, a 1% chance of belonging to Class B, and a
14% chance of belonging to Class C. This unseen point would therefore be classified as a member
of Class A. Here is another example in which the calculations are explicitly shown, see the
figure and table below.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/bayes_classifier/bayesian_example2.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
&lt;/figure&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Point #&lt;/th&gt;
      &lt;th&gt;Class&lt;/th&gt;
      &lt;th&gt;Likelihood&lt;/th&gt;
      &lt;th&gt;Prior&lt;/th&gt;
      &lt;th&gt;Marginal&lt;/th&gt;
      &lt;th&gt;Posterior (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;0.9754&lt;/td&gt;
      &lt;td&gt;0.2188&lt;/td&gt;
      &lt;td&gt;0.2227&lt;/td&gt;
      &lt;td&gt;95.813&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;0.0298&lt;/td&gt;
      &lt;td&gt;0.3125&lt;/td&gt;
      &lt;td&gt;0.2227&lt;/td&gt;
      &lt;td&gt;4.181&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;0.0000&lt;/td&gt;
      &lt;td&gt;0.4688&lt;/td&gt;
      &lt;td&gt;0.2227&lt;/td&gt;
      &lt;td&gt;0.006&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;0.0000&lt;/td&gt;
      &lt;td&gt;0.2188&lt;/td&gt;
      &lt;td&gt;0.0507&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;0.1621&lt;/td&gt;
      &lt;td&gt;0.3125&lt;/td&gt;
      &lt;td&gt;0.0507&lt;/td&gt;
      &lt;td&gt;100.000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;0.0000&lt;/td&gt;
      &lt;td&gt;0.4688&lt;/td&gt;
      &lt;td&gt;0.0507&lt;/td&gt;
      &lt;td&gt;0.000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;A&lt;/td&gt;
      &lt;td&gt;0.2168&lt;/td&gt;
      &lt;td&gt;0.2188&lt;/td&gt;
      &lt;td&gt;0.0724&lt;/td&gt;
      &lt;td&gt;65.394&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;B&lt;/td&gt;
      &lt;td&gt;0.0564&lt;/td&gt;
      &lt;td&gt;0.3125&lt;/td&gt;
      &lt;td&gt;0.0724&lt;/td&gt;
      &lt;td&gt;24.309&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;C&lt;/td&gt;
      &lt;td&gt;0.0159&lt;/td&gt;
      &lt;td&gt;0.4688&lt;/td&gt;
      &lt;td&gt;0.0724&lt;/td&gt;
      &lt;td&gt;10.297&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Recall that the priors are calculated as the fraction of training objects in
the class to the total number of training objects; therefore, since each class
has a different number of points, the priors are different (Class A has 70,
Class B has 100, and Class C has 150). The likelihood is calculated as the
two-dimensional Gaussian using the mean and covariance matrix for that particular
class, which is determined from the training data. The marginal is simply the sum
of likelihood&lt;script type=&quot;math/tex&quot;&gt;\times&lt;/script&gt;prior for all three classes. The posterior, or the predictive
probability of interest, is calculated as likelihood&lt;script type=&quot;math/tex&quot;&gt;\times&lt;/script&gt;prior/marginal.
The point labeled as 3, which originally has been sampled from the Class C probability distribution,
is determined to be a member of Class A, which is arguably incorrect.&lt;/p&gt;

&lt;p&gt;The classification may be summarized by the following histograms. One may scan through
the feature space and calculate the posteriors for all classes; the maximum posterior
then determines the classification, see the figure below where the light blue represents a situation
where Class A is a maximum, tan represents when Class B is a maximum, and dark red represents
when Class C is a maximum. Each class has a small gradient effect which is a result of
the value of the class posterior when it’s determined to be the maximum. Lastly,
see Fig. 3 for a summary of the three classes where in this case the color weighting
is equal to the posterior probability.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/bayes_classifier/which_class_is_max_better.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
    &lt;figcaption&gt;Fig. 2 - Summarizing the classification within the feature space, where
    light blue means that the Class A posterior is a maximum, tan is Class B, and dark red
    is Class C. The binning in both directions is 1/150. Note that this depends
    on the training data, which is randomly generated, and does not reflect the exact
    nature of the above plots (but it is close).&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/bayes_classifier/bayesian_summary.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
    &lt;figcaption&gt;Fig. 3 - Summarizing the classification within the feature space. The
    color weighting is the posterior probability for the three classes, and white represents
    a posterior of zero (or extremely close to it). Notice how these plots are
    slightly different than Fig. 2, and is a consequence of fitting to randomly
    generated training data (seen in the top-left panel) which slightly alters the predictive
    mean/covariance matrix. In this case, Class A sampled some
    data that extend above the Class C negatively correlated data, and explains the
    features seen in the top-right panel.&lt;/figcaption&gt;
&lt;/figure&gt;</content><summary type="html">A probabilistic classifier using Gaussian class-conditionals</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Quadtree</title><link href="http://localhost:4000/blog/quadtree/" rel="alternate" type="text/html" title="Quadtree" /><published>2018-10-09T00:00:00-04:00</published><updated>2018-10-09T00:00:00-04:00</updated><id>http://localhost:4000/blog/quadtree</id><content type="html" xml:base="http://localhost:4000/blog/quadtree/">&lt;p&gt;The quadtree post serves two purposes: to learn JavaScript in order to make interactive
code run on the site, and to gain more experience with tree structures. I had been playing
around with binary trees and graphs, and wanted to implement a visual
representation of a tree structure. The quadtree idea and implementation may be seen by many sources
(Wiki has decent pseudocode). In complete analogy to a binary tree, a quadtree
may be recursively built but now 4 children nodes are possible.
Of importance to me, though, is that the structure allows for a rapid search or query
of neighboring balls, which is useful in the context of 2D collisions for example.
The naive algorithm to search for nearest neighbors is &lt;i&gt;O(N&lt;sup&gt;2&lt;/sup&gt;)&lt;/i&gt;,
&lt;i&gt;i.e.&lt;/i&gt; for each ball, one must check the position of every other ball which is a
double for-loop algorithm (with some minor optimizations that are possible). The
quadtree may be used in order to return a sub-list of balls that are in the vicinity
of the ball in question; the result may then be used for collision detection, which
is a significant improvement relative to the naive approach for many balls.
There are other approaches to solve this problem more efficiently,
and I am not claiming that the use of a quadtree is the best.
Note that I did not implement a collision routine as I’ve already worked this out in a
previous post.&lt;/p&gt;</content><summary type="html">Animating the tree-structure for 2D collisions</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Scenic screen saver</title><link href="http://localhost:4000/blog/scenic-screen-saver/" rel="alternate" type="text/html" title="Scenic screen saver" /><published>2018-06-15T00:00:00-04:00</published><updated>2018-06-15T00:00:00-04:00</updated><id>http://localhost:4000/blog/scenic-screen-saver</id><content type="html" xml:base="http://localhost:4000/blog/scenic-screen-saver/">&lt;figure&gt;
  &lt;img src=&quot;/images/scenery/scenery.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=lxAgjiuRBeQ&quot;&gt;A video may be seen here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;The original goal was to simulate a rope; however, once the rope/swing was made
then the project unintentionally grew to something much larger.
Since I had just created a random tree generator, I thought it would be
fun to anchor the simulated rope on the branches. Even cooler, let’s move
the rope according to some fictitious wind force which introduced the
need for the Perlin noise generator. The project grew to include a simulated sunset/sunrise (which I have
been wanting to do for quite some time now), lightning bugs, an owl, blades of grass,
stars, and a moon to make the scenery more interesting.&lt;/p&gt;

&lt;p&gt;Ropes may be simulated by connecting masses to springs, and then one or both ends of the rope-object may be
anchored. When both sides are anchored and all masses are in the presence of gravity,
the proper catenary curve is reproduced which is a
canonical calculus of variations result (hyperbolic cosine). Implementing a rope simulation in C++ is
well described by &lt;a href=&quot;http://nehe.gamedev.net/tutorial/rope_physics/17006/&quot;&gt;this site&lt;/a&gt;,
and will not be described in detail here. In short, though, the idea is to build a Mass class
where the position may be updated as&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;void Mass::simulate(float dt){
  fVel.x += (fForce.x / fMass) * dt;
  fVel.y += (fForce.y / fMass) * dt;
  fPos.x += fVel.x * dt;
  fPos.y += fVel.y * dt;
}

void Mass::applyForce(sf::Vector2f force){
  fForce.x += force.x;
  fForce.y += force.y;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where dt is the intrinsic delta time in between frames (SFML provides sf::Clock
and sf::Time classes for this purpose). Then, a Spring class is made where one
spring is connected to two Mass-objects, and a spring force &lt;script type=&quot;math/tex&quot;&gt;-k(x-x')&lt;/script&gt;
is used to constrain/connect the two masses; in this case, &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; is an
adjustable parameter, &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the distance between the two masses, and &lt;script type=&quot;math/tex&quot;&gt;x'&lt;/script&gt;
is the length of the spring in absence of forces. This is where the Mass::applyForce(sf::Vector2f)
comes in handy, which may also be used for gravity, air friction, or anything else that comes to mind
to alter the behavior. A Rope-object is then built
out of many Spring classes to realistically simulate the movement of a rope. 
The rope is continuously perturbed by a
&lt;a href=&quot;http://libnoise.sourceforge.net/&quot;&gt;Perlin coherent noise generator&lt;/a&gt;
to simulate the effects of wind. Note that the anchor points need to be
updated according to the perturbations of the fractal tree.&lt;/p&gt;

&lt;p&gt;The generation of the sunset was computationally expensive, and I learned
how to do it &lt;a href=&quot;http://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/simulating-sky/simulating-colors-of-the-sky&quot;&gt;here&lt;/a&gt;, which includes Mie and Rayleigh scattering
models. The calculation needs to be performed for many angles (depending on the
location of the sun relative to a viewer on Earth) in order to simulate
a sunset/sunrise. I generated roughly 360 images, and the screen saver cycles through each
image approximately every 0.5 seconds.&lt;/p&gt;

&lt;p&gt;The remainder of the scenery, excluding the tire and
&lt;a href=&quot;http://rosprites.blogspot.com/2012/05/classes-other.html&quot;&gt;owl sprites&lt;/a&gt;,
were made completely with SFML vertex arrays. The random tree fractal generator is
described in a previous post; the blades of grass use the
same fundamental construction
(built using a class that accepts 2 points for the bottom/top center points of a box,
a width parameter, and a parameter to change the width ratio of the top relative to the bottom)
in order to create the gradient effect. The alpha component of a blade is adjusted once the
scenery is determined to be dark. The moon is actually 100 sf::CircleShapes
where the alpha component is adjusted to give the glow effect. The bugs are
black circles which randomly glow (the glow lasts for a time that is randomly determined
by a Gaussian distribution); the glow becomes more frequent during the night time.
The cricket noise was downloaded from
&lt;a href=&quot;http://soundbible.com/tags-crickets.html&quot;&gt;here&lt;/a&gt;, and the volume increases
at night. Just about the entire scenery (excluding the sprites and the sunset) are
randomly generated and positioned, either using a flat or normal distribution using C++11’s
random-class, which yields variations of the same screen-saver idea.&lt;/p&gt;</content><summary type="html">Includes simulated ropes, a sunset/sunrise model, and Perlin coherent noise</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Perceptron</title><link href="http://localhost:4000/blog/perceptron/" rel="alternate" type="text/html" title="Perceptron" /><published>2018-06-01T00:00:00-04:00</published><updated>2018-06-01T00:00:00-04:00</updated><id>http://localhost:4000/blog/perceptron</id><content type="html" xml:base="http://localhost:4000/blog/perceptron/">&lt;figure&gt;
  &lt;img src=&quot;/images/perceptron/perceptron_top.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
    &lt;figcaption&gt;Fig. 1 - Linearly separable data and the result of the perceptron
    algorithm in which the weights are determined using &lt;i&gt;K&lt;/i&gt;-fold cross validation and averaging.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;The perceptron is a simple model for supervised learning of binary classifiers.
The algorithm makes use of
ROOT, which has vector and matrix classes for convenience.
I generated two sets of pseudo-training data,
which are normally distributed of varying means and sigmas, that are classified
as blue circle = -1 and red triangle = +1, see Figure 1.
One may think of a system that needs to classify a coin based on radius and mass for example.
The classification function is
&lt;script type=&quot;math/tex&quot;&gt;S = \boldsymbol{w} \cdot \boldsymbol{x} + b&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;
are the weights to be learned and are initially assigned random values,
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt; are the features (mass and radius in this case), and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is a
threshold or bias constant. A common practice is to absorb &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; into the zeroth
component of &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt; and to set the zeroth component of &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt;
to 1, which results in a vector &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt; of size 3.
Therefore, &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w} \cdot \boldsymbol{x}&lt;/script&gt;
is simply a vector dot-product.
If &lt;script type=&quot;math/tex&quot;&gt;S&gt;0&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
S&lt;0 %]]&gt;&lt;/script&gt;) and the data point in question is a red triangle (blue circle),
then the weight vector &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt; has done a successful classification.
On the other hand, if &lt;script type=&quot;math/tex&quot;&gt;S&gt;0&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
S&lt;0 %]]&gt;&lt;/script&gt;) and the data point in question
is a blue circle (red triangle), then the weights (which determine the slope and intercept
of the linear classification function) need to be updated. The update rule is:
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}(t+1) = \boldsymbol{w}(t) + \eta * h(t) * \boldsymbol{x}(t)&lt;/script&gt;,
where &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; is a learning-rate constant chosen to be 0.05, &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the iteration number,
&lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; is the correct classification of the point in question (either +1 or -1),
and &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt; represents the features. In other words, incorrect classifications
wiggle the linear decision boundary to a position where data are correctly classified,
&lt;i&gt;i.e.&lt;/i&gt; data of one type falls on one side of the line, and the remaining data
falls on the other side.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/perceptron/updating_w.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
    &lt;figcaption&gt;Fig. 2 - The weight parameters are updated until a decision boundary has been found,
                which may be seen by the black line. Note that many decision boundaries exist,
		see &lt;i&gt;K&lt;/i&gt;-folds below for a more sophisticated approach.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Assuming that the data is linearly separable, then the algorithm needs to examine the
full data set many times. For each iteration, a for-loop
through the full data set is implemented in which &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; is calculated for each
data point. If a data point is incorrectly classified, then update &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;.
If the for-loop scans through all data &lt;i&gt;and&lt;/i&gt; no updates to &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;
are performed, then successfully exit. A linear line may then be constructed with a
slope of &lt;script type=&quot;math/tex&quot;&gt;-w_{1}/w_{2}&lt;/script&gt; and an intercept of &lt;script type=&quot;math/tex&quot;&gt;-w_{0}/w_{2}&lt;/script&gt;, which are the components
of the &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt; vector. See Fig. 2 for an example of the updating, and
note that the algorithm simply stops once a correct classification of the training data
has been reached.&lt;/p&gt;

&lt;p&gt;In order to improve upon the previous approach, the training data may be sub-divided
many times in order to calculate &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt; repeatedly for slightly different
data sets and initial conditions.
For this example, one data point is removed the from the training
data set, and the algorithm is implemented using the &lt;script type=&quot;math/tex&quot;&gt;N-1&lt;/script&gt; data points;
this procedure is repeated until every data point has been removed (while retaining the
&lt;script type=&quot;math/tex&quot;&gt;N-1&lt;/script&gt; size, so put back the previously removed point), and the average &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;
is taken as the final decision boundary. This approach is sometimes referred to as
&lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;-fold cross validation; see Fig. 3 where the left panel displays
the training (bold line is the result of the &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;-fold averaging approach). The
right panel shows how well the classifier does with unseen data, which in this
case an efficiency of greater than 99% is observed.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/perceptron/example_new.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
    &lt;figcaption&gt;Fig. 3 - Training (left panel) and a prediction (right). The naive and &lt;i&gt;K&lt;/i&gt;-folds
    averaging approach are compared in the training phase of the algorithm.
    &lt;/figcaption&gt;
&lt;/figure&gt;</content><summary type="html">A simple linear classification algorithm</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Random Tree Generator</title><link href="http://localhost:4000/blog/tree-fractal-v2/" rel="alternate" type="text/html" title="Random Tree Generator" /><published>2018-05-20T00:00:00-04:00</published><updated>2018-05-20T00:00:00-04:00</updated><id>http://localhost:4000/blog/tree-fractal-v2</id><content type="html" xml:base="http://localhost:4000/blog/tree-fractal-v2/">&lt;figure&gt;
  &lt;img src=&quot;/images/tree_fractal/tree_road_trip_small.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig. 1 - A fractal tree using my &quot;random tree generator&quot;
  with the black/white option turned ON. The tree has been placed on top of a photograph
  of the Badlands that I took during a road trip (summer of 2012).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;/p&gt;
&lt;h2 id=&quot;improvements&quot;&gt;Improvements&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://youtu.be/yIIv5fRgoSY&quot;&gt;The tree fractal program may be seen by this video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;p&gt;I realized that I wanted to play with the fractal tree a bit more… My first
post may be seen &lt;a href=&quot;/blog/tree-fractal/&quot;&gt;here&lt;/a&gt;
which includes an overview of the algorithm. The code has been extended/reformulated
to handle a variety of options in addition to a GUI interface; this is how the tree
gets interesting which I ignored the first time!
While I have written sliders, buttons, and number entries from scratch, I used the C++
&lt;a href=&quot;https://root.cern.ch/root/htmldoc/guides/users-guide/WritingGUI.html&quot;&gt;ROOT GUI widgets&lt;/a&gt;
for this project which has been an absolute pleasure.
Furthermore, instead of simply representing a branch as a line, I included an option to
toggle between lines or user-adjustable trapezoids, where the latter is designed
to give the effect that the diameter of a branch/trunk decreases with height.
The parameters that are now adjustable using GUI widgets are the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt; the number of iterations to display &lt;/li&gt;
&lt;li&gt; the number of branches to generate at a particular node (previously I only did 2, but
     now it is possible to choose between [1,10])&lt;/li&gt;
&lt;li&gt; the generation angle which controls the angle of branches in the &lt;i&gt;n+1&lt;/i&gt; iteration
     relative to the &lt;i&gt;n&lt;sup&gt;th&lt;/sup&gt;&lt;/i&gt; iteration &lt;/li&gt;
&lt;li&gt; the length and width of the trunk which affects subsequent branches as well &lt;/li&gt;
&lt;li&gt; the trapezoidal gradient of a branch &lt;/li&gt;
&lt;li&gt; the branch width and length relative to the trunk &lt;/li&gt;
&lt;li&gt; an asymmetry angle offset which yields trees that are biased towards one direction&lt;/li&gt;
&lt;li&gt; an offset for the starting point of a branch &lt;/li&gt;
&lt;li&gt; an option to turn the background back/white which automatically renders the
     tree white/black, respectively &lt;/li&gt;
&lt;li&gt; an option to draw the branches in order/reverse (sometimes branches cover previous
     iterations, therefore this option is for convenience) &lt;/li&gt;
&lt;li&gt; an option to change the tree color palette (15 options) which automatically applies
     a pretty gradient ranging from the trunk to the smallest branch. &lt;/li&gt;
&lt;li&gt; the ability to randomly generate a tree in which the above parameters are tweaked
     according to various probability distributions. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/tree_fractal/assortment_nsplits_small.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
  &lt;figcaption&gt;Fig. 2 - Going clockwise and starting at the top left, the number of branches
  to generate at a new node is 3, 5, 8, and 9. The trunk length and branch ratio relative
  to the trunk have been adjusted to get a
  pretty picture (hence why GUI widgets are wonderful for this project).&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;The ability to increase the number of branches at a node significantly changes the
structure of the fractal, &lt;i&gt;e.g.&lt;/i&gt; see Fig. 2 which displays the unexpected
generation of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Sierpinski_triangle&quot;&gt;Sierpinski triangle&lt;/a&gt;
among others; this behavior was a huge surprise for me. See Fig. 3 for an example
of a tree with 4 branches generated a every node; it resembles the previous post but
much fuller. Note that the number of branch iterations needs to be monitored when tweaking
the number of branches to generated, otherwise too many lines are drawn and the program
inevitably crashes.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/tree_fractal/nsplit_4_thickness.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
  &lt;figcaption&gt;Fig. 3 - The number of branches generated at each node is 4, and the fractal
  has been generated with a modest angle of approximately 100 degrees.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&quot;random-tree-generator&quot;&gt;Random Tree Generator&lt;/h2&gt;
&lt;p&gt;The random tree generator is useful for creating trees that visually look “natural” in
structure, see Fig. 1. There are a great deal of parameters to randomize; however, this
is what I did:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt; change the number of branches to be generated at a particular node 10%
     of the time using a flat random generator - I do not want the change to be large, and
     I adjusted this number by +/- 1 &lt;/li&gt;
&lt;li&gt; 10% of the time, change the length of the branch using a Gaussian distribution
     where the standard deviation is small, specifically 10% of the length of the branches
     that would normally be generated &lt;/li&gt;
&lt;li&gt; 33.3% of the time, change the generation angle with a Gaussian distribution of
     standard deviation equal to 10 degrees&lt;/li&gt;
&lt;li&gt; change the branch starting position with a Gaussian distribution (std dev = 10% of
     the normal branch length, and we must take an absolute value to avoid strange behavior)
     40% of the time &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These parameters can all be changed, obviously, but too many changes yields wild results; therefore,
it is best to control how often a Gaussian (or flat) wiggle occurs. See Fig. 4 and
the figures beyond for examples of the random generator with various options turned ON/OFF.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/tree_fractal/random_v2.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
  &lt;figcaption&gt;Fig. 4 - An example of the random tree generator; note that Fig. 1 also
  is randomly generated, but in black and white (and placed on another image using
  &lt;a href=&quot;https://www.gimp.org/&quot;&gt;GIMP&lt;/a&gt;).
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/tree_fractal/more_trees_white.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/tree_fractal/random_v6.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
  &lt;figcaption&gt;Fig. 5 - The original tree used in Fig. 1.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;</content><summary type="html">A significant upgrade to the previous tree fractal post</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Fractal Ferns</title><link href="http://localhost:4000/blog/fractal-fern/" rel="alternate" type="text/html" title="Fractal Ferns" /><published>2018-05-12T00:00:00-04:00</published><updated>2018-05-12T00:00:00-04:00</updated><id>http://localhost:4000/blog/fractal-fern</id><content type="html" xml:base="http://localhost:4000/blog/fractal-fern/">&lt;figure&gt;
  &lt;img src=&quot;/images/fern/mod_ferns_rotsmall.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig. 1 - Selected color schemes of a modified Barnsley fern. The
  fern is plotted in a two dimensional histogram, and the color gradient
  depends on the bin content in a log scale.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;/p&gt;
&lt;h2 id=&quot;barnsley-fern&quot;&gt;Barnsley Fern&lt;/h2&gt;
&lt;p&gt;The generation of ferns is surprisingly easy compared to other fractals that I’ve attempted;
however, getting pretty color schemes was not obvious (at least for me).
The fern idea is not mine, but a summary may be seen
&lt;a href=&quot;https://en.wikipedia.org/wiki/Barnsley_fern&quot;&gt;here&lt;/a&gt;. The technique falls
under the category of an &lt;i&gt;iterated function system&lt;/i&gt; which differs
from the generation of a tree fractal or a Koch snowflake. In this
case, very specific matrix transformations are performed on a point, and the type of transformation
is dictated by a flat probability distribution. For example, the canonical fern
is the so-called Barnsley fern, which may be generated by considering some
initial point defined to be &lt;script type=&quot;math/tex&quot;&gt;x_n&lt;/script&gt;. The initial point undergoes linear
transformations of the type &lt;script type=&quot;math/tex&quot;&gt;x_{n+1} \rightarrow Mx_n + b&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;2\times2&lt;/script&gt;
linear transformation and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is a translational term. The Barnsley fern
is generated by the following choices of &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
	&amp;= \begin{pmatrix} 0.00 &amp; 0.00 \\ 0.00 &amp; 0.16 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ 0.00 \end{pmatrix} \hspace{1cm} P=0.01, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
        &amp;= \begin{pmatrix} 0.85 &amp; 0.04 \\ -0.04 &amp; 0.85 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ 0.16 \end{pmatrix} \hspace{1cm} P=0.85, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
       &amp;= \begin{pmatrix} 0.20 &amp; -0.26 \\ 0.23 &amp; 0.22 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ 1.60 \end{pmatrix} \hspace{1cm} P=0.07, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
       &amp;= \begin{pmatrix} -0.15 &amp; 0.28 \\ 0.26 &amp; 0.24 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ 0.44 \end{pmatrix} \hspace{1cm} P=0.07. \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Which transformation to use depends on a flat random probability distribution
denoted by &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;, so for example the first transformation is only applied 1% of
the time while the second is applied 85%. The result is the famous Barnsley fern which
may be seen by Fig. 2; the fern is generated by starting with some initial
point and applying the formula with the defined probabilities many times, say &lt;script type=&quot;math/tex&quot;&gt;10^{5} - 10^{8}&lt;/script&gt;.
This is an &lt;script type=&quot;math/tex&quot;&gt;xy&lt;/script&gt; plot, though, not a histogram which is important when it comes to
assigning colors to the fern. Note that this fern does not look exactly the same as Fig. 1
which are generated using the prescription found
&lt;a href=&quot;http://www.home.aone.net.au/~byzantium/ferns/fractal.html&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
       &amp;= \begin{pmatrix} 0.00 &amp; 0.00 \\ 0.00 &amp; 0.20 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ -0.12 \end{pmatrix} \hspace{1cm} P=0.01, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
        &amp;= \begin{pmatrix} 0.845 &amp; 0.035 \\ -0.035 &amp; 0.82 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ 1.60 \end{pmatrix} \hspace{1cm} P=0.85, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
       &amp;= \begin{pmatrix} 0.20 &amp; -0.31 \\ 0.255 &amp; 0.245 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ 0.29 \end{pmatrix} \hspace{1cm} P=0.07, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
       &amp;= \begin{pmatrix} -0.15 &amp; 0.24 \\ 0.25 &amp; 0.20 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ 0.68 \end{pmatrix} \hspace{1cm} P=0.07, \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;which is quite similar to the Barnsley parameters with the exception of the translational terms.
An animation of the fern development is seen by Fig. 4.
The trick to getting the colors observed in Fig. 1 is to plot the fern in a two-dimensional histogram
which contains more information than a simple xy plot. Why? Well, certain points are chosen more
frequently than others, and this information may be captured by a histogram. For example, I used a
large number of bins for the x and y variables, roughly 900 to 1000, and then the
logarithm of the number events in each bin may be used to defined the rgb value.
The ferns in Fig. 1 are displayed in a
log-scale which highlights small differences in bin counts; furthermore, the color gradient observed
in the leaves (there is a proper name for this) occurs naturally.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/fern/fern_thumbnail_v3.png&quot; alt=&quot;&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;
  &lt;figcaption&gt;Fig. 2 - The Barnsley fern plotted as an xy scatter plot. Information is lost
  here in regards to color schemes; therefore, I used a two dimensional histogram and a
  log scale as seen in Fig. 1.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Another pretty fern may be generated using the following values for &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; (found on Wiki)
and may be seen by Fig. 3:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
       &amp;= \begin{pmatrix} 0.00 &amp; 0.00 \\ 0.00 &amp; 0.25 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.00 \\ -0.40 \end{pmatrix} \hspace{1cm} P=0.02, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
        &amp;= \begin{pmatrix} 0.95 &amp; 0.005 \\ -0.005 &amp; 0.93 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} -0.002 \\ 0.50 \end{pmatrix} \hspace{1cm} P=0.84, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
       &amp;= \begin{pmatrix} 0.035 &amp; -0.20 \\ 0.16 &amp; 0.04 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} -0.09 \\ 0.02 \end{pmatrix} \hspace{1cm} P=0.07, \\
\begin{pmatrix} x_{n+1} \\ y_{n+1} \end{pmatrix}
       &amp;= \begin{pmatrix} -0.04 &amp; 0.20 \\ 0.16 &amp; 0.04 \end{pmatrix}
       \begin{pmatrix} x_n \\ y_n   \end{pmatrix} +
       \begin{pmatrix} 0.083 \\ 0.12 \end{pmatrix} \hspace{1cm} P=0.07. \\
\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/fern/fern_skinny_2.png&quot; alt=&quot;&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
  &lt;figcaption&gt;Fig. 3 - Another mutant variation of the Barnsley fern.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/fern/fern.gif&quot; alt=&quot;&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
  &lt;figcaption&gt;Fig. 4 - An animation of the fern seen in Fig. 1 where the iteration is in
  powers of 10, &lt;i&gt;i.e.&lt;/i&gt; 1 to &lt;i&gt;10&lt;sup&gt;8&lt;/sup&gt;&lt;/i&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;</content><summary type="html">Generating fractal ferns using iterated function systems</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Convex Hulls</title><link href="http://localhost:4000/blog/convex_hull/" rel="alternate" type="text/html" title="Convex Hulls" /><published>2018-03-20T00:00:00-04:00</published><updated>2018-03-20T00:00:00-04:00</updated><id>http://localhost:4000/blog/convex_hull</id><content type="html" xml:base="http://localhost:4000/blog/convex_hull/">&lt;h2 id=&quot;overview-of-algorithm&quot;&gt;Overview of Algorithm&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;image left&quot;&gt;&lt;img src=&quot;http://localhost:4000/images/convex/convex_for_span.png&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;
The idea is simple: find the minimum number of data points which represent the perimeter convex polygon
about a data set. There are many variations of this algorithm and the applications can
be critical; for example, an introductory machine learning problem is to construct
a boundary (a straight line with 2 weights in the simplest case) between two linearly
separable data sets, where linearly separable means that there exists some line that
perfectly differentiates the two data sets (all the data from one set fall on one side of
the line while all the data from the other set fall on the other side).
However, how does one know if the two data sets are &lt;i&gt;linearly separable&lt;/i&gt;?
One method is to apply the convex hull algorithm to both data sets individually in order
to construct two polygons, then the polygons may be tested for overlaps in order to confirm (or not)
linear separability. Here are the major components of my convex hull algorithm,
which at the core consists of calculating smallest angles between carefully chosen vectors
(See Fig. 1 for an example in which the data are labeled):
&lt;ol&gt;
&lt;li&gt; Generate a data set. I generated points based on a Gaussian distribution with a mean
     and sigma of 0.5 and 0.25, respectively, as I did not want to points to wander too far.&lt;/li&gt;
&lt;li&gt; Each point needs to be indexed using some book-keeping integer for tracking purposes.&lt;/li&gt;
&lt;li&gt; Find the algorithm's starting point - I chose the point with the minimum y value, and
     if there are duplicates then I choose the point with the lowest x and y values, or the
     bottom-left point; this point by definition is part of the hull.
     The starting point in Fig. 1 is labeled as 8.&lt;/li&gt;
&lt;li&gt; Make a hull container which will be used to keep track of the points making
     up the perimeter polygon (or the hull), and insert the starting point as by definition this
     point must be apart of the hull.
     Also, it is useful to make a storage container that contains all
     the data points, which allows for element removal as we traverse the hull;
     this is the container that is used to find points to add to the hull.&lt;/li&gt;
&lt;li&gt; Construct a vector using information from the hull container;
     the first iteration consists of the first hull point 8 and the origin. The
     direction is important as we want to calculate minimum angles. &lt;/li&gt;
&lt;li&gt; Find the minimum angle between a test-case and the hull vector (origin and point 8);
     the test-point with the minimum angle is the next hull point, which
     is labeled by 1 and may now be removed from storage. &lt;/li&gt;
&lt;li&gt; Repeat. The next hull vector is built by points 8 and 1, then the minimum angle
     between this vector and a test-vector is the next hull point;
     in this case, it is given by point 7. Note that this strategy
     allows angles to be calculated by a simple dot product.&lt;/li&gt;
&lt;li&gt; Continue until we reach the starting point, hence the utility of the book-keeping device &lt;/li&gt;     
&lt;/ol&gt;
&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/convex/example.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig 1. - An example of the algorithm in which the starting point is labeled as 8. Note
  that the numbers are simply a book-keeping device and should not be interpreted as anything more.
  The ordering of the next points are 1, 7, 2, 0, 3, and 8 (return to the starting point),
  which collectively define the hull polygon.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/convex/example_of_overlap2.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig 2. - An example of a situation in which the two data sets, differentiated by red squares and black triangles, are not linearly separable.
  The convex hull algorithm may be used to detect this behavior.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&quot;overlapping-convex-polygons&quot;&gt;Overlapping Convex Polygons&lt;/h2&gt;
&lt;p&gt;What if we are interested in the overlapping region between two convex hulls? Assuming
that the hulls overlap, then the boolean AND operation results in another convex polygon.
Determining the overlap can be difficult, but here is what I chose to do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Break the convex polygon P&lt;sub&gt;1&lt;/sub&gt; of &lt;i&gt;N&lt;/i&gt; vertices into sub-triangles - I wrote a routine
    that takes the hull coordinates (arrays/vectors of x,y points) and returns
    &lt;i&gt;N-2&lt;/i&gt; triangles (in the form of arrays of x,y points). &lt;/li&gt;
&lt;li&gt; Loop through the other hull polygon, denoted by P&lt;sub&gt;2&lt;/sub&gt; which has &lt;i&gt;M&lt;/i&gt; vertices,
     and check to see if any of the x,y points live in the sub-triangles of P&lt;sub&gt;1&lt;/sub&gt;.
     The following function was used for this purpose:
&lt;pre&gt;&lt;code&gt;bool in_triangle(point p, double* x, double* y){
  point p0(x[0],y[0],-1); // triangle vertex
  point p1(x[1],y[1],-1); // triangle vertex
  point p2(x[2],y[2],-1); // triangle vertex
  double A = 0.5 * (-p1.Y() * p2.X() + p0.Y() * (-p1.X() + p2.X())
		    + p0.X() * (p1.Y() - p2.Y()) + p1.X() * p2.Y());
  double sign = A &amp;lt; 0 ? -1 : 1;
  double s = (p0.Y() * p2.X() - p0.X() * p2.Y() + (p2.Y() - p0.Y()) * p.X()
	      + (p0.X() - p2.X()) * p.Y()) * sign;
  double t = (p0.X() * p1.Y() - p0.Y() * p1.X() + (p0.Y() - p1.Y()) * p.X()
	      + (p1.X() - p0.X()) * p.Y()) * sign;

  return s &amp;gt; 0 &amp;amp;&amp;amp; t &amp;gt; 0 &amp;amp;&amp;amp; (s + t) &amp;lt; 2 * A * sign;	
}
&lt;/code&gt;&lt;/pre&gt;
where point p is the point to test in order to determine if it lives
in a triangle of x and y coordinates denoted by p0, p1, and p2; if so, then
the function returns true, otherwise false.&lt;/li&gt;
&lt;li&gt;The inverse needs to be checked, &lt;i&gt;i.e.&lt;/i&gt; check to see if any
points of polygon P&lt;sub&gt;1&lt;/sub&gt; live in P&lt;sub&gt;2&lt;/sub&gt;.&lt;/li&gt;
&lt;li&gt;Intersection points between P&lt;sub&gt;1&lt;/sub&gt; and P&lt;sub&gt;2&lt;/sub&gt; also need
    to be tracked if one desires the overlapping polygon; this can
    be achieved by simple vector intersection.&lt;/li&gt;
&lt;li&gt;All these points need to be tracked and put into an array in CW or CCW order
    in order to build a convex polygon.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The result of the above logic may be seen by Fig. 3. Note that I am only
concerned about the hull polygons, not the data within the hull (which only matter
during the construction of the hull).
The top-left panel displays the two hulls, denoted by H&lt;sub&gt;red&lt;/sub&gt; and H&lt;sub&gt;blk&lt;/sub&gt;.
The top-right panel displays the result of the sub-triangularization method,
and explicitly shows which hull vertices were detected using the above code snippet.
In other words, the red squares are detected in H&lt;sub&gt;blk&lt;/sub&gt; while the black
triangle is a point of the hull H&lt;sub&gt;blk&lt;/sub&gt; that has been detected in H&lt;sub&gt;red&lt;/sub&gt;.
The bottom panel displays the overlapping result in blue, which is another convex polygon.
The markers are coded by shape where the meaning of the square and triangle markers
has just been discussed; the circle markers, however, represent intersection points
between the two convex hulls and must be considered when constructed the overlapping hull.
Note that in the context of linear separability, none of this is necessary; one only needs
to check to see if a point of hull H&lt;sub&gt;blk&lt;/sub&gt; exists within H&lt;sub&gt;red&lt;/sub&gt;
and vice versa.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;/images/convex/convex_overlap_detection.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig 3 - An overview of the convex hull overlap detection algorithm, which
  is represented by the blue convex polygon.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;/p&gt;</content><summary type="html">An algorithm to find the perimeter of a data set, and includes convex polygon overlap detection</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Word Search</title><link href="http://localhost:4000/blog/word-search/" rel="alternate" type="text/html" title="Word Search" /><published>2018-01-04T00:00:00-05:00</published><updated>2018-01-04T00:00:00-05:00</updated><id>http://localhost:4000/blog/word-search</id><content type="html" xml:base="http://localhost:4000/blog/word-search/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://youtu.be/YexCgQyk7TE&quot;&gt;The game may be seen here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am not particularly interested in word searches, but for some reason
I found myself making a generator. The word search game is complete and user-interactive,
and the entire project has been coded using the bare-bones tools of SFML which
requires significant &lt;i&gt;behind-the-scenes&lt;/i&gt; routines.
The code is mostly book-keeping tasks, but a decent strategy needs to be developed when it
comes to placing many strings of varying size in a grid.
First, I need a dictionary, or a text file
with hundreds of thousands of words; I chose
&lt;a href=&quot;http://www.math.sjsu.edu/~foster/dictionary.txt&quot;&gt;this one&lt;/a&gt; which explains
why some of the words to find are not commonly accepted words in my opinion (but who cares this
is trivial to change). The code imports
300 randomly selected words from the dictionary, which are organized based on the string length 
allowing for a method to choose between easy, medium, and hard words (typically
smaller words are harder to find). The lattice is generated using SFML
vertex arrays, and the word placement algorithm is designed as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with the strings with the largest length as the geometric constraints are stricter.&lt;/li&gt;
&lt;li&gt;Randomly select a starting position in the grid and generate a list of possible directions
based on geometric considerations; there are 8 possibilities (vertical, horizontal,
and the diagonals). Randomly select a possible direction, and place the first word.&lt;/li&gt;
&lt;li&gt;Continue the procedure for all words to find, typically 15 words (5 from each hardness category).&lt;/li&gt;
&lt;li&gt;When a new word is randomly positioned, an overlap check is performed. If an overlap exists,
check to see if the character is the same. If it is then keep it, otherwise discard the word
and move on to another. There are many smarter approaches, I imagine, but this is what I how
I positioned many words in a grid.&lt;/li&gt;
&lt;li&gt;Fill the in the empty spaces with randomly generated letters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A cheat button is achieved
by simply toggling between two sprites (a checked/unchecked box). Mouse events are handled
using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sf::Mouse::isButtonPressed(sf::Mouse::Left)
sf::Event::MouseButtonPressed	
sf::Event::MouseMoved
sf::Event::MouseButtonReleased
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;logic methods. For example, when the mouse button is clicked &lt;i&gt;once&lt;/i&gt; the cell must turn red;
when the mouse moves, the cells turn red in vertical/horizontal/diagonal preferences
which is achieved using simple vectors and angular cuts. When the mouse button is released,
a method must be invoked to check if the highlighted region is in fact a word on the list. If
it is, turn the blocks green, otherwise reset the blocks and cue the audio for failure.
The words to find list are sorted by string length, and a strike-out effect is achieved
by using vertex arrays. The audio clips are handled by SFML’s audio module which is
simple to use for this task.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/word_search/word_search_thumbnail.png&quot; alt=&quot;&quot; width=&quot;99%&quot; height=&quot;99%&quot; /&gt;
&lt;/figure&gt;</content><summary type="html">An interactive word search generator and game</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000" /></entry><entry><title type="html">Mona Lisa</title><link href="http://localhost:4000/blog/mona-lisa/" rel="alternate" type="text/html" title="Mona Lisa" /><published>2017-08-28T00:00:00-04:00</published><updated>2017-08-28T00:00:00-04:00</updated><id>http://localhost:4000/blog/mona-lisa</id><content type="html" xml:base="http://localhost:4000/blog/mona-lisa/">&lt;p&gt;&lt;i&gt;Disclaimer&lt;/i&gt;: I got the idea from &lt;a href=&quot;https://rogerjohansson.blog/2008/12/07/genetic-programming-evolution-of-mona-lisa/&quot;&gt;this blogging site&lt;/a&gt;,
but the implementation/algorithms are mine. The blogger calls this a &quot;genetic algorithm&quot; but it's just a simple hill-climbing
method defined by some &amp;chi;&lt;sup&gt;2&lt;/sup&gt;, a common technique in my work.
Additionally, I used the bare-bones &lt;a href=&quot;https://www.sfml-dev.org/&quot;&gt;SFML library&lt;/a&gt; which required
significant image processing and filtering methods to be developed; this has been done on purpose for my own education!
While the results are pleasing, the algorithm is not practical (at least mine). An animation of the algorithm is located at the bottom.
&lt;/p&gt;

&lt;h2 id=&quot;concept&quot;&gt;Concept&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;image right&quot;&gt;&lt;img src=&quot;http://localhost:4000/images/mona_lisa/original.jpg&quot; alt=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The goal is to mimic or replicate an input image using polygons, or triangles in this case.
The main components of the algorithm are as follows:
&lt;ol&gt;
&lt;li&gt;Have an input image that we wish to copy, &lt;i&gt;e.g.&lt;/i&gt; the Mona Lisa&lt;/li&gt;
&lt;li&gt;Randomly generate an initial set of &lt;i&gt;N&lt;/i&gt; triangles, typically 50-500&lt;/li&gt;
&lt;li&gt;Define an appropriate &amp;chi;&lt;sup&gt;2&lt;/sup&gt; function, often referred to as a loss or fitness function, which
    will allow us to quantify the difference between the triangle image and the input image &lt;/li&gt; 
&lt;li&gt;Apply a &lt;q&gt;mutation&lt;/q&gt;, &lt;i&gt;i.e.&lt;/i&gt; randomly tweak a characteristic of the triangle like a vertex
    coordinate or an RGBA value&lt;/li&gt;
&lt;li&gt;Calculate the &amp;chi;&lt;sup&gt;2&lt;/sup&gt; between the mutated image and the input; if the function determines
    that the mutation is more similar to the input then accept the change. Otherwise, discard it, and apply another mutation&lt;/li&gt;
&lt;li&gt;Repeat until the loss function does not change significantly.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With low-level C++ libraries, the difficulty here is to develop the appropriate routines to handle the image processing.
For example, &lt;i&gt;N&lt;/i&gt; triangles are randomly generated in which there is significant overlap; therefore, RGBA values
are being added together in a complicated manner. While more sophisticated methods can be developed,
I chose to draw the triangles on a canvas in order to visually see
the result of a mutation (for example, a coordinate mutation can affect a significant fraction of the RGBA of the mutation image).
In order to process the mutated image, the code takes a screen shot of the mutation and then breaks it apart into
a vector such that the RGBA components may be easily compared to the input image.&lt;/p&gt;

&lt;p&gt;Let's define the word &lt;i&gt;mutation&lt;/i&gt; clearly. A mutation only applies to the triangle image, and in general
represents some random tweaking of triangle attributes. In this case, one triangle has three vertices and each
vertex has two coordinates and an RGBA value; this is 6 parameters per vertex for a grand total of 18 per triangle! Therefore,
the number of parameters to tweak making up the triangle image goes like &lt;i&gt;N*18&lt;/i&gt; where &lt;i&gt;N&lt;/i&gt; is the number
of triangles. The difficulty is to decide how much to randomly tweak a parameter. If the mutation is too small then
it is easy for the code to get stuck in a local minima, but if it is too large then the mutation rate is compromised.
I used a flat random number generator for all parameters, and kept the coordinate mutations loose while keeping tight
mutations on the color components. There is a lot of room for improvement in these choices, I think.&lt;/p&gt;

The &amp;chi;&lt;sup&gt;2&lt;/sup&gt; function is defined to be the square difference of R,G,B,A components for all pixels between the input
and mutated image, and is normalized by &lt;i&gt;(N pixels)*4*256*256&lt;/i&gt; such that
the loss function &lt;i&gt;L = 1-&lt;/i&gt;&amp;chi;&lt;sup&gt;2&lt;/sup&gt; yields a number less than or equal to 1 where 1 is a perfect match.
This can be computationally slow if you have large images as the code is simply brute force. The heart of the
algorithm depends on this function as the newly mutated image gets compared to the input image via the loss function. If
the loss function is calculated to be better than the previous triangle image, then the mutation is kept; otherwise,
the code will discard the mutation and randomly try again.
The mutation rate, &lt;i&gt;i.e.&lt;/i&gt; the number of accepted mutations over the total number of mutation attempts,
at first is acceptable but then quickly becomes linear resulting in a slow algorithm (still fun though).&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;/images/mona_lisa/mutation_rate.png&quot; alt=&quot;&quot; height=&quot;99%&quot; width=&quot;99%&quot; /&gt;
  &lt;figcaption&gt;Fig. 1 - The normalized loss (L=1-&amp;chi;&lt;sup&gt;2&lt;/sup&gt;) as a function of mutation number.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;image left&quot;&gt;&lt;img src=&quot;/images/mona_lisa/mona_lisa_rs.gif&quot; alt=&quot;&quot; height=&quot;20%&quot; width=&quot;20%&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to make convergence faster, I gridded up the input image and found the average color of each grid.
I generated 300 triangles with randomly placed vertices and calculated the vertex coordinate average which dictates
the triangle's initial color based off the average grid colors.
I let the code do 1e6 mutation attempts (as opposed to successes)
in which only 23,721 mutations were accepted, which is a mutation rate of 2.4% if I assume linearity.
The best loss is L=0.9966, and it remained this way for quite some time as the slope is so small (see Fig. 1 at large mutations).
The quality of the image does converge, meaning that more computational time is most likely futile.
The results may be greatly improved by considering polygons (I've seen ellipses too), but the number of parameters
to tweak is now &lt;i&gt;N*V*6&lt;/i&gt; where &lt;i&gt;V&lt;/i&gt; is the number of vertices.
Also, the use of triangles leads to sharp edges in the output image. This can be smoothed out by developing
filtering techniques, &lt;i&gt;e.g.&lt;/i&gt; a Gaussian blur, and I spent a great deal of time learning how to apply
convolution matrices or image masks (google 'kernel image processing') from scratch. 
&lt;/p&gt;</content><summary type="html">A hill climbing method to mimic an input image with randomly generated triangles</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/mona_lisa/mona_lisa.jpg" /></entry></feed>
