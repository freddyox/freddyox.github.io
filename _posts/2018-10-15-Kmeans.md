---
layout:  post
title:  "K-means clustering"
date:    2018-10-15
excerpt: "An unsupervised clustering algorithm"
comments: true
image: ""
cimage: "/images/kmeans/Kmeans_thumbnail_v1.png"
categories: [machine learning,ROOT]
---

<figure>
  <img src="/images/kmeans/4Means_example_success.png" alt="" height="100%" width="100%">
  <figcaption>Fig. 1 - Unlabeled data (left) is clustered or grouped into
                       4 categories (right), which are arbitrarily labeled as
		       A, B, C, and D. An animation of the algorithm may be seen below.
		       Lastly, the algorithm is applied to Old Faithful geyser data in order to
		       make an educated guess on the wait-time for the next eruption.</figcaption>
</figure>
<p></p>

## Summary
1. [K-means clustering](#kmeans)
2. [Animation of the algorithm](#animation)
3. [Application to Old Faithful](#old_faithful)

## Overview of the problem <a class="anchor" id="kmeans"></a>
We are given $$N$$ objects each of which have features $$\boldsymbol{x}=\left(x_1, x_2\right)$$ but
no class labels. Therefore, an unsupervised algorithm such as $$K$$-means clustering
may be utilized in order to group data of similar features; the output of my implementation
may be seen by Fig. 1.
The phrase "similar features" is quite ambiguous, however the
most naive interpretation may be thought in terms of distance. In other words, similar objects
are defined to be objects that are close together in terms of the square distance:

$$
D^{2}=(\boldsymbol{x}_i - \boldsymbol{x}_j)^{T}(\boldsymbol{x}_i - \boldsymbol{x}_j)
 = (x_{i1} - x_{j1})^{2} + (x_{i2} - x_{j2})^{2}, 
$$

where the simplification is a result of only working with two features, and the superscript
$$T$$ refers to vector-transpose. In order to automate this procedure for a data set
that looks like the left-panel of Fig. 1, then a cluster needs to be defined more clearly.

## Clustering algorithm  <a class="anchor" id="animation"></a>
In $$K$$-means, there are $$k$$ clusters (<i>e.g.</i> 4 clusters in Fig. 1)
that are defined by the feature-space mean using the data points constituting the cluster
in the calculation, <i>i.e.</i> in the $$(x_1,x_2)$$ space one needs to
calculate the average in both variables but only using data points that have been determined
to be within the cluster.
The argument is circular; we need a mean-point to define a cluster and yet the cluster needs
points to calculate the mean. In order to overcome this strange logic, the mean-vector
$$\boldsymbol{\mu}$$ for the $$k^{th}$$ cluster is defined as

$$
\boldsymbol{\mu}_k = \frac{\sum_n z_{nk} \boldsymbol{x}_n}{\sum_n z_{nk}},
$$

where $$z_{nk}$$ is a binary variable which is equal to 1 if the data
object is a part of cluster $$k$$, and zero otherwise;
this is simply a weighted average, and keeps track of how many objects
are in cluster $$k$$. Randomly initialize $$\boldsymbol{\mu}$$ for the $$k$$ clusters.
It is best to keep the randomization within the bounds of the data, otherwise a
cluster may not develop. Randomly assign all unlabeled data to a cluster; therefore, a book-keeping
device (an integer ranging from 0 to 3 in Fig. 1 for example)
is required for each data object to track cluster assignment.
The following procedures are then performed iteratively:
1. For each data object, calculate the distance between $$\boldsymbol{x}_n$$ and
   $$\boldsymbol{\mu}_k$$ for the $$k$$ clusters, <i>i.e.</i> find $$k$$ that minimizes
   $$D=\sqrt{(\boldsymbol{x}_n - \boldsymbol{\mu}_k)^{T}(\boldsymbol{x}_n - \boldsymbol{\mu}_k)}$$.
   The data point is assumed to be a member of the cluster
   that yields the smallest distance, <i>i.e.</i> update the data-object's book-keeping
   device to reflect this change, if any. If no updates are required, then exit the iterative-loop.
2. If we are here, then the book-keeping device of at least one data-object has been updated; therefore
   update $$\boldsymbol{\mu}_k$$ by recalculating the weighted-averages to reflect this change.
3. Return to procedure 1.
<figure>
  <img src="/images/kmeans/animation_loop.gif" alt="" height="99%" width="99%">
  <figcaption>Fig. 2 - An animation of the algorithm where the black stars represent
  the means of the five clusters.</figcaption>
</figure>
<p></p>
In the above example, the unlabeled data has been classified or organized
into five clusters (blue square, green triangle, red circle, purple
upside-down triangle, copper cross). The black stars, which represent $$\boldsymbol{\mu}_k$$
or the mean of the $$k^{th}$$ cluster, move around due to the above update rule; the algorithm
stops once the data-object's book-keeping device is no longer updated.

## Application to Old Faithful <a class="anchor" id="old_faithful"></a>
Data from more than 270 observations on the Old Faithful geyser
<a href="https://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat">may be found here</a>.
We are given the duration of the current eruption in addition to the waiting time
for the next eruption. The data distribution may be seen by Fig. 3, which is clustered
into two groups, labeled long and short, using K-means. In short, the eruption time is either
short $$(\lesssim 3 \textrm{min})$$ or long $$(\gtrsim 3 \textrm{min})$$,
which means that we have to wait a
short (roughly 55 min) or long (roughly 80 min) time until the next eruption. Note this
data is almost 30 years old, and consequently current predictions may differ. The uncertainty
in this prediction is roughly $$2-3 \sigma$$, which comes out to be about $$\pm 10-15$$ minutes;
so get there early just in case!

<figure>
  <img src="/images/kmeans/old_faithful_summary.png" alt="" height="99%" width="99%">
  <figcaption>Fig. 3 - Applying K-means (using Python's scikit-learn for practice) to Old Faithful data.
              The algorithm finds two clusters, labeled as long and short, which
	      may be used to predict the time one needs to wait until the next eruption given
	      the duration of the current eruption.</figcaption>
</figure>