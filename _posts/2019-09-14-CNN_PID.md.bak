---
layout:  post
title:  "Particle Classification"
date:    2019-09-14
excerpt: "Particle identification using convolutional neural networks"
comments: true
image: ""
cimage: "/images/cnn_pid/thumbnail.png"
categories: [machine learning,deep learning,simulation]
---
<figure>
  <img src="/images/cnn_pid/event_animation_v2.gif" alt="" height="100%" width="100%">
  <figcaption>Fig. 1 - Cross-sectional view of the BigHand detector during a neutron
  event. A neutron enters the detector from the left and deposits energy (red) as
  it propagates through the detector planes. The goal is to use low-level detector
  information in order to classify the particle species.</figcaption>
</figure>
<p></p>
# Summary
1. [BigHand Detector Package](#bighand)
2. [Classification](#class)
   1. [Physics Inspired Method](#old)
   2. [Machine Learning Methods](#new)
      1. [Data Preparation](#data)
      2. [Random Forest](#rf)
      3. [Convolutional Neural Network](#cnn)

Understanding how neutrons and protons are classified within the BigHand detector package
represented an important analysis task during my PhD thesis; the original method will be
discussed briefly. I have revisited the classification problem roughly one year later except
in this case I will be using machine learning tools, specifically a random forest model and
a convolutional neural network (CNN) approach. The newer methods produce outstanding results
and have greatly increased the classification accuracy relative to the original machinery.
By far the most difficult task was acquiring the appropriate data, and conditioning it
such that a Random Forest/CNN may be used.

# BigHand Detector Package <a class="anchor" id="bighand"></a>
Let me start with this: the BigHand detector is complicated and analysis tasks
are generally a mess. I am not going to discuss the detector or geometry in detail as I have
exhaustively described the detector
<a href="https://opencommons.uconn.edu/dissertations/2045/">in my thesis</a> and a Phys. Rev. C
document that is currently being prepared. The geometry may be seen by Fig. 2.

<figure>
  <img src="/images/cnn_pid/ND_schematic_v1.png" alt="" height="100%" width="100%">
  <figcaption>Fig. 2 - A cross sectional of the BigHand detector displaying the locations
  of the scintillating bars in addition to the lead and iron. There are 9 planes in total:
  7 (N1-N7) handle nucleon momentum reconstruction via a clustering procedure and 2 (V1/V2)
  are devoted to particle identification via a physics-inspired method.</figcaption>
</figure>
<p></p>
BigHand was an amalgamation of five types of scintillating bars, two (three) for the
veto (neutron) layers which are labeled as V1 and V2 (N1-N7) within Fig. 2, respectively.
Bars of a particular type were organized into cassettes, or modular units consisting of
ten or less scintillating bars for structural integrity, which were stacked to form a plane.
With the exception of the second veto plane, all cassettes had 1 cm of lead or iron on the
target side to increase the probability of inducing a hadronic shower. The veto planes
had 48+48 scintillating bars; from front to back, the seven neutron layers labeled N1-N7
consisted of 29, 25, 30, 25, 45, 45, and 45 organic plastic scintillating bars. In total,
the detector had 340 modules, and this structure will serve as the basis of our data input
to the convolutional neural network.

# Classification <a class="anchor" id="class"></a>
## Original Method  <a class="anchor" id="old"></a>
The goal of BigHand was two-fold: 1) reconstruct the momentum of a recoiling proton or neutron
(collectively referred to as a nucleon) and 2) particle classification, <i>i.e.</i> algorithmically
decide whether the particle was a proton or a neutron. Detector planes N1-N7 were responsible for
momentum reconstruction via an energy deposition clustering procedure; this cluster is classified as
proton or neutron by exploiting time and spatial correlations between the cluster and
hits within the veto layers (V1 and V2). The underlying principle takes advantage of the fact that
charged particles like protons emit light (or scintillate) within scintillating materials while
uncharged particles like neutrons do not. Therefore, if a cluster
is formed but no correlated hits are found within the veto layers, then this event is classified
as a neutron. On the other hand, if a cluster is found in addition to correlated hits within
the veto layers, then this particle is classified as a proton. See Fig. 3 for a summary.

<figure>
  <img src="/images/cnn_pid/MC_ND_events.png" alt="" height="75%" width="75%">
  <figcaption>Fig. 3 - Summarizing the particle classification concept. The fundamental
  idea rests upon the fact that charged particles like protons produce light or scintillation
  within the modules of the BigHand detector. The presence of a cluster within planes (N1-N7)
  in addition to spatially correlated hits within the V1/V2 layers results in the classification
  of proton.
  </figcaption>
</figure>
<p></p>

This method is physically intuitive and satisfying as we exploit processes that we think
we understand very well. We make the assumption that neutrons do not deposit energy in
the veto layers (we threshold this, but still this comment is valid). In practice, however,
this turns out to be a mediocre assumption for a couple of reasons: 1) the shielding
upstream significantly complicates things and 2) we are operating the detector
in a noisy (or imperfect) environment. This leads to a mediocre neutron classification accuracy
as our cluster-veto hits are assumed to be spatially correlated; therefore,
if a neutron deposits energy in the veto layers
and a cluster is formed, then this particle is assigned proton by the analysis...
In other words, this approach misidentifies these types of events every time.
This analysis correctly classifies protons $$\sim 97$$% of the time while the neutron
accuracy is $$\sim 55$$%, which is just barely better than random guessing. The
goal of this re-analysis is to do better with nucleon classification!

## Machine Learning Methods <a class="anchor" id="new"></a>
### Data Preparation <a class="anchor" id="data"></a>
Acquiring and preparing the data for this machine learning analysis is by far the most
difficult and time consuming part. When I analyze experimental data from the BigHand
detector, I do not have access to the truth, <i>i.e.</i> the labels of the particles.
We try our best by implementing the above physics-inspired scheme to identify particles,
but I do not and will never have the ground-truth.

I did, however, spend a great deal of time during my PhD building a simulation that
replicates the experimental data. The simulation utilizes Monte Carlo methods and
frankly is beyond the scope of this post; the <a href="https://geant4.web.cern.ch/">GEANT4</a>
simulation framework has been used which is the standard in the field.
In order to get the particle labels, two separate
simulations are performed: 1) protons are generated using the
<a href="https://arxiv.org/abs/1401.2959">ESEPP generator</a> which incorporates 
the lowest-order QED (quantum electrodynamics) radiative corrections to the
Rosenbluth cross section among other things, and 2) a fictitious neutron target that follows
the lowest-order QED rules of electron-neutron scattering.
Both particles are directed towards BigHand with an angular coverage that populates
the detector acceptance that is consistent with what is expected/observed during experiment.
The simulated energy deposition in BigHand scintillator bars
is modeled in multiple ways, <i>i.e.</i> the gain coefficients of the
photomultiplier tubes are incorporated in addition to the
bar-dependent attenuation of the scintillation.
The simulated analysis incorporates the realistic trigger scheme
(this is what trips the electronics for data-taking in real life),
and all simulated physics variables faithfully reproduce what is observed during experiment.
For more information, one must seek the details within my thesis.

For each event that initiates a BigHand trigger, the particle label and energy deposition
of all BigHand bars are outputed resulting in a table of approximately
180k rows (corresponding to events) and 341 columns (340 correspond
to the energy deposition within the BigHand bars and 1 corresponds to the particle label).
Roughly 61% of this data corresponds to neutron events. This dataset represents
the starting point for the machine learning methods.
We will be using carefully chosen low-level detector information, namely
energy deposition in scintillator post detector triggering, in order to build
a proton/neutron classifier.

<b>Note: the majority of the work
required to obtain this dataset has been superficially discussed as I do not want
to get too deep in the physics-related weeds. Additionally, the simulation that I
have developed has been rigorously tested against the real data during the final
2 years of my PhD.</b>

### Random Forest Classifier <a class="anchor" id="rf"></a>
The features are the energy deposition of all BigHand modules and have been normalized
by the maximum energy deposition in that particular event. I found that the results
may be improved by removing the last two planes of BigHand from my feature space; this
is not surprising to me because protons/neutrons generally do not propagate through the
entire detector at this entire scale. The target or label is the proton/neutron
simulation that has been briefly described above. Here are some details about the
dataset:

{% highlight python %}
Neutron data  - row, col: 108,635, 341
 Proton data  - row, col:  69,115, 341
All data      - row, col: 177,750, 341
Analysis data - row, col: 177,750, 250 (N6 and N7 removed)
Ratio of neutrons to protrons: 61%
{% endhighlight %}

As usual, the data is goes through a train-test-split (30% for testing) and is
fit to a RandomForestClassifier with all default parameters with the exception
of n_estimators=250. The prediction on the test dataset yields the following:
{% highlight python %}
Total Accuracy: 91.17%

Proton  Accuracy:   17944/20725 = 86.58%
Proton  Inaccuracy:  2781/20725 = 13.42%
Neutron Accuracy:   30675/32600 = 94.10%
Neutron Inaccuracy:  1925/32600 = 5.90%
{% endhighlight %}

In other words, the Random Forest model correct classifies protons (neutrons)
87% (94%) of the time which is pretty remarkable. The most important features
by detector plane may be summarized by the following figure:

<figure>
  <img src="/images/cnn_pid/rf_featimportance_acc.png" alt="" height="100%" width="100%">
  <figcaption>Fig. 4 - The feature importance of the Random Forest model organized
  by BigHand detector plane, <i>i.e.</i> feature importance within a particular
  plane are summed and then normalized by the number of modules within that plane.
  </figcaption>
</figure>
<p></p>

Fig. 4 indicates that the veto layers (recall that the physics-inspired methodology
relied heavily upon the response within the veto layers for particle classification)
and neutron plane N1 are the most important in regards to this classification task.
Planes deeper in the detector become decreasingly important which is not a surprise
to me. This model will serve as a baseline for the convolutional approach, in other words
can a CNN do better than 87% and 94% for protons and neutrons, respectively?

### Convolutional Neural Network <a class="anchor" id="cnn"></a>